<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenResponses - Project Deep Dive | Gunnar Hostetler</title>
    <meta name="description" content="SwiftUI-powered AI assistant for OpenAI Responses API with computer use, code interpreter, and MCP integrations.">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
:root {
    --bg-primary: #0a0a0f; --bg-secondary: #12121a; --bg-card: #1a1a24;
    --text-primary: #fff; --text-secondary: #a0a0b0;
    --accent: #6366f1; --accent-light: #6366f199;
    --border-color: #2a2a3a;
}
* { margin: 0; padding: 0; box-sizing: border-box; }
body { font-family: 'Inter', sans-serif; background: var(--bg-primary); color: var(--text-primary); line-height: 1.6; }
.container { max-width: 1200px; margin: 0 auto; padding: 0 2rem; }
.project-nav { position: fixed; top: 0; left: 0; right: 0; background: rgba(10,10,15,0.95); backdrop-filter: blur(10px); padding: 1rem 2rem; display: flex; justify-content: space-between; align-items: center; z-index: 1000; border-bottom: 1px solid var(--border-color); }
.back-link { color: var(--text-secondary); text-decoration: none; display: flex; align-items: center; gap: 0.5rem; }
.back-link:hover { color: var(--accent); }
.nav-links { display: flex; gap: 1.5rem; }
.nav-links a { color: var(--text-secondary); text-decoration: none; font-size: 0.9rem; }
.nav-links a:hover { color: var(--accent); }
.project-hero { padding: 8rem 0 4rem; background: linear-gradient(180deg, var(--bg-secondary), var(--bg-primary)); text-align: center; }
.project-hero h1 { font-size: 3.5rem; font-weight: 800; margin-bottom: 1rem; background: linear-gradient(135deg, #fff, var(--accent)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
.hero-subtitle { font-size: 1.25rem; color: var(--text-secondary); max-width: 700px; margin: 0 auto 2rem; }
.hero-actions { display: flex; justify-content: center; gap: 1rem; flex-wrap: wrap; }
.btn { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.75rem 1.5rem; border-radius: 8px; text-decoration: none; font-weight: 500; transition: all 0.2s; }
.btn-primary { background: var(--accent); color: white; }
.btn-primary:hover { filter: brightness(1.1); transform: translateY(-2px); }
.btn-secondary { background: var(--bg-card); color: var(--text-primary); border: 1px solid var(--border-color); }
.btn-secondary:hover { border-color: var(--accent); }
.btn-appstore { background: #000; color: white; border: 1px solid #333; }
.btn-appstore:hover { background: #1a1a1a; }
.section { padding: 5rem 0; }
.section-alt { background: var(--bg-secondary); }
.section h2 { font-size: 2rem; margin-bottom: 2rem; text-align: center; }
.features-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1.5rem; }
.feature-card { background: var(--bg-card); padding: 1.5rem; border-radius: 12px; border: 1px solid var(--border-color); transition: border-color 0.2s; }
.feature-card:hover { border-color: var(--accent); }
.feature-card h3 { color: var(--accent); margin-bottom: 0.5rem; font-size: 1.1rem; }
.feature-card p { color: var(--text-secondary); font-size: 0.95rem; }
.tech-stack { display: flex; flex-wrap: wrap; justify-content: center; gap: 0.75rem; margin-top: 2rem; }
.tech-tag { background: var(--bg-card); padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.85rem; border: 1px solid var(--border-color); }
.project-footer { padding: 2rem 0; text-align: center; border-top: 1px solid var(--border-color); }
.project-footer a { color: var(--accent); text-decoration: none; }
.sync-time { font-size: 0.85rem; color: var(--text-secondary); margin-top: 0.5rem; }
@media (max-width: 768px) { .project-hero h1 { font-size: 2.5rem; } .nav-links { display: none; } .hero-actions { flex-direction: column; align-items: center; } }
    </style>
</head>
<body>
    <nav class="project-nav">
        <a href="../../index.html" class="back-link"><i class="fas fa-arrow-left"></i> Portfolio</a>
        <div class="nav-links">
            <a href="#features">Features</a>
            <a href="https://github.com/Gunnarguy/OpenResponses" target="_blank"><i class="fab fa-github"></i></a>
        </div>
    </nav>

    <header class="project-hero">
        <div class="container">
            <h1>OpenResponses</h1>
            <p class="hero-subtitle">SwiftUI-powered AI assistant for OpenAI Responses API with computer use, code interpreter, and MCP integrations.</p>
            <div class="hero-actions">
                <a href="https://github.com/Gunnarguy/OpenResponses" class="btn btn-primary" target="_blank">
                    <i class="fab fa-github"></i> View on GitHub
                </a>
                
            </div>
            <div class="tech-stack"><span class="tech-tag">SwiftUI</span><span class="tech-tag">Swift</span><span class="tech-tag">OpenAI</span><span class="tech-tag">RAG</span><span class="tech-tag">Apple Intelligence</span><span class="tech-tag">MCP</span><span class="tech-tag">Computer Use</span><span class="tech-tag">Code Interpreter</span><span class="tech-tag">MVVM</span></div>
        </div>
    </header>

    <main class="docs-main">
      <div class="container">
        <div class="docs-hero">
          <p class="docs-kicker">Snapshot ‚Ä¢ Jan 17, 2026</p>
          <h1>OpenResponses Docs</h1>
          <p>
            OpenAI Responses playground with streaming tools, MCP connectors,
            and a production SwiftUI client.
          </p>
          <div class="docs-actions">
            <a class="btn-link" href="https://github.com/Gunnarguy/OpenResponses" target="_blank" rel="noopener">GitHub Repo</a>
            <a class="btn-link" href="https://apps.apple.com/us/app/openresponses/id6757338355" target="_blank" rel="noopener">App
              Store</a>
            <a class="btn-link" href="../../index.html#projects">Back to Projects</a>
          </div>
        </div>

        <div class="docs-layout">
          <aside class="docs-sidebar">
            <div class="docs-sidebar-title">Sections</div>
            <ul class="docs-nav">
              <li><a href="#doc-readme">README</a></li>
              <li><a href="#doc-roadmap">Roadmap</a></li>
              <li><a href="#doc-architecture">Architecture</a></li>
            </ul>
            <div class="docs-status">
              Generated from GitHub on Jan 17, 2026.
            </div>
          </aside>

          <article class="md-content">
            <section class="docs-section" id="doc-readme">
              <div class="docs-section-header">
                <h2>README</h2>
                <div class="docs-section-meta">
                  <a href="https://github.com/Gunnarguy/OpenResponses/blob/main/README.md" target="_blank" rel="noopener">View source</a>
                  </div>
                  </div>
                  <div class="docs-section-body">
                    <h1 id="openresponses">OpenResponses</h1>
                <p>
                  SwiftUI-powered AI assistant for the OpenAI Responses API
                  featuring computer use, code interpreter, file search, image
                  generation, and MCP integrations‚Äîall wrapped in a
                  production-ready iOS experience with deep observability and
                  safety rails.
                </p>
                <p>
                  <a href="https://github.com/Gunnarguy/OpenResponses/actions/workflows/ios-ci.yml"><img alt="iOS CI"
                      src="https://github.com/Gunnarguy/OpenResponses/actions/workflows/ios-ci.yml/badge.svg" /></a>
                  <a href="https://github.com/Gunnarguy/OpenResponses/actions/workflows/release-check.yml"><img alt="Release Checks"
                      src="https://github.com/Gunnarguy/OpenResponses/actions/workflows/release-check.yml/badge.svg" /></a>
                  <a href="LICENSE"><img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-blue.svg" /></a>
                </p>
                <blockquote>
                  <p>
                    <strong>Status ‚Äî November¬†2025:</strong> Phase¬†1 is
                    complete. OpenResponses ships with local conversation
                    storage, full Responses tool support, and the Minimal Viable
                    App-Store Submission (MVAS) checklist. Phase¬†2 focuses on
                    Conversations API migration and cross-device sync.
                  </p>
                  </blockquote>
                  <hr />
                  <h2 id="table-of-contents">Table of Contents</h2>
                  </ul>
                  </hr>
                  <h2 id="documentation-hub">Documentation Hub</h2>
                  <ul>
                  <li>
                  <li>
                    <a href="#initial-configuration">Initial Configuration</a>
                  </li>
                  <li>
                    <a href="#toolbox-at-a-glance">Toolbox at a Glance</a>
                  </li>
                  <li>
                    <a href="#privacy-safety-and-compliance">Privacy, Safety, and Compliance</a>
                  </li>
                  <li>
                    <a href="#testing--quality-gates">Testing &amp; Quality Gates</a>
                  </li>
                  <li><a href="#release-workflow">Release Workflow</a></li>
                  <li><a href="#documentation-hub">Documentation Hub</a></li>
                  <li><a href="#roadmap-snapshot">Roadmap Snapshot</a></li>
                  <li><a href="#contributing">Contributing</a></li>
                  <li><a href="#support">Support</a></li>
                  <li><a href="#license">License</a></li>
                  </ul>
                  <hr />
                  <h2 id="overview">Overview</h2>
                <p>
                  OpenResponses is an end-to-end iOS, iPadOS, and macOS
                  (Catalyst) client for the OpenAI Responses API. It targets
                  developers and advanced users who need:
                </p>
                <ul>
                  <li>
                    Full coverage of the current tool surface (computer use,
                    code interpreter, file/vector search, image generation, MCP
                    connectors).
                  </li>
                  <li>
                    Rich observability‚Äîstreaming analytics, reasoning trace
                    playback, and API inspectors that make debugging and demos
                    effortless.
                  </li>
                  <li>
                    Enterprise-ready safeguards‚ÄîKeychain credential storage,
                    explicit approval flows for automation, and a minimal
                    privacy footprint.
                  </li>
                  </ul>
                <p>
                  The app follows a productized workflow: everything you need to
                  test, ship, and submit to the App Store‚Äîincluding privacy
                  docs, tracking scripts, and the MVAS tracker‚Äîis built into the
                  repository.
                </p>
                <hr />
                <h2 id="core-features">Core Features</h2>
                <ul>
                  <li>
                    <strong>Model Playground:</strong> Live model catalogue with
                    compatibility gating, preset management, and advanced
                    request controls (streaming flags, prompt cache IDs,
                    reasoning toggles).
                  </li>
                  <li>
                    <strong>Observability Surface:</strong> Streaming activity
                    feed, live token usage, ‚ÄúAssistant Thinking‚Äù trace viewer,
                    analytics events, and structured logging for every tool
                    event.
                  </li>
                  <li>
                    <strong>Tooling Portfolio:</strong> Computer use with safety
                    approvals, code interpreter with artifact viewer,
                    multi-vector file search, direct file and image attachments,
                    Notion/MCP connectors, custom function calls.
                  </li>
                  <li>
                    <strong>Knowledge Workflows:</strong> Vector store
                    management flows, file conversion pipeline, and document
                    picker enhancements built on
                    <code>FileConverterService</code>.
                  </li>
                  <li>
                    <strong>Native Shell:</strong> SwiftUI UI with accessibility
                    support, keyboard shortcuts, share sheets, prompt library,
                    onboarding, and settings tuned for fast iteration.
                  </li>
                  </ul>
                  <hr />
                  <h2 id="architecture">Architecture</h2>
                <p>
                  OpenResponses follows MVVM with dependency injection through
                  <code>AppContainer</code>.
                </p>
                <ul>
                  <li>
                    <strong>Views:</strong> SwiftUI views such as
                    <code>ChatView</code>, <code>MessageBubbleView</code>, and
                    modular settings/onboarding screens.
                  </li>
                  <li>
                    <strong>View Models:</strong>
                    <code>ChatViewModel</code> orchestrates conversations,
                    state, and tool execution; extensions such as
                    <code>ChatViewModel+Streaming</code> handle 40+ streaming
                    event types.
                  </li>
                  <li>
                    <strong>Services:</strong> <code>OpenAIService</code> wraps
                    the Responses API, <code>ComputerService</code> automates
                    the computer-use browser,
                    <code>ConversationStorageService</code> persists local
                    history, <code>KeychainService</code> stores secrets, and
                    compatibility helpers gate tooling per model.
                  </li>
                  <li>
                    <strong>Data Models:</strong> Rich types for streaming
                    events, function calls, computer-use actions, artifacts, and
                    reasoning traces keep decoding resilient and expressive.
                  </li>
                  </ul>
                  <blockquote>
                  <p>
                    Dive deeper in <code>docs/CASE_STUDY.md</code> for component
                    diagrams, request flows, and design decisions.
                  </p>
                  </blockquote>
                  <hr />
                  <h2 id="getting-started">Getting Started</h2>
                  <h3 id="prerequisites">Prerequisites</h3>
                  <ul>
                    <li>Xcode 16.1 (or newer)</li>
                    <li>macOS Sonoma</li>
                    <li>An OpenAI API key (sk-‚Ä¶ project key)</li>
                  </ul>
                  <h3 id="clone-open">Clone &amp; Open</h3>
                <div class="codehilite">
                  <pre><span></span><code>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/Gunnarguy/OpenResponses.git
<span class="nb">cd</span><span class="w"> </span>OpenResponses
open<span class="w"> </span>OpenResponses.xcodeproj
</code></pre>
</div>

                <h3 id="build-targets">Build Targets</h3>
                <ul>
                  <li>
                    <strong>OpenResponses (iOS/iPadOS):</strong> Run on
                    simulator or device.
                  </li>
                  <li>
                    <strong>OpenResponses (macOS Catalyst):</strong> Build/run
                    via ‚ÄúMy Mac (Designed for iPad)‚Äù scheme.
                  </li>
                  </ul>
                  <hr />
                  <h2 id="initial-configuration">Initial Configuration</h2>
                  <ol>
                    <li>Launch the app.</li>
                  <li>
                    Complete onboarding (3 screens summarizing capabilities and
                    key requirements).
                  </li>
                  <li>
                    When prompted, paste your OpenAI API key. It is stored in
                    the iOS Keychain (<code>KeychainService</code>) and never
                    checked into source control.
                  </li>
                  <li>
                    Use Settings ‚Üí General to toggle streaming, published
                    prompts, and prompt cache IDs.
                  </li>
                  <li>
                    Enable tools (code interpreter, computer use, file search,
                    MCP) in Settings ‚Üí Tools. Each capability enforces
                    additional confirmation flows as required.
                  </li>
                  </ol>
                <p>
                  Secrets are intentionally absent from the repo. Run
                  <code>python3 scripts/secret_scan.py</code> anytime to
                  validate.
                </p>
                <hr />
                <h2 id="toolbox-at-a-glance">Toolbox at a Glance</h2>
                <table>
                  <thead>
                    <tr>
                      <th>Capability</th>
                      <th>Details</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td><strong>Computer Use</strong></td>
                      <td>
                        Navigate/click/scroll automation with safety approval
                        sheets, blank-page recovery, screenshot attachments, and
                        status updates.
                      </td>
                      </tr>
                      <tr>
                        <td><strong>Code Interpreter</strong></td>
                      <td>
                        Sandboxed Python execution with artifact viewer, status
                        heartbeats, and result summarization.
                      </td>
                      </tr>
                      <tr>
                        <td><strong>File Search &amp; Vector Stores</strong></td>
                      <td>
                        Upload files, manage vector stores, toggle file search
                        per prompt, and configure rankers or thresholds.
                      </td>
                      </tr>
                      <tr>
                        <td><strong>Image Generation</strong></td>
                      <td>
                        Trigger image creation with optional detail level
                        control and inline previews.
                      </td>
                      </tr>
                      <tr>
                        <td><strong>MCP Connectors</strong></td>
                      <td>
                        Register local/remote MCP servers, inspect tools, and
                        gate usage through approval UI with Keychain-backed
                        auth.
                      </td>
                      </tr>
                      <tr>
                        <td><strong>Prompt Library</strong></td>
                      <td>
                        Save and reuse prompt presets including reasoning/model
                        settings and safety identifiers.
                      </td>
                      </tr>
                      <tr>
                        <td><strong>Observability</strong></td>
                      <td>
                        Activity feed, streaming status chips, token usage
                        counters, API inspector, debug console, and analytics
                        hooks.
                      </td>
                      </tr>
                      </tbody>
                      </table>
                      <hr />
                <h2 id="privacy-safety-and-compliance">
                  Privacy, Safety, and Compliance
                </h2>
                <ul>
                  <li>
                    <strong>Credentials:</strong> API keys and integration
                    tokens live only in the Keychain. No secrets ship with the
                    app or reside on disk.
                  </li>
                  <li>
                    <strong>Data Residency:</strong> Conversations and
                    attachments stay on device until you explicitly send them to
                    OpenAI or an MCP tool.
                  </li>
                  <li>
                    <strong>Permissions:</strong> The app currently requests
                    Photos, Files, Calendars, Contacts, Reminders, and Local
                    Network usage descriptions. Camera, microphone, speech
                    recognition, and location are intentionally excluded in
                    v1.0.0.
                  </li>
                  <li>
                    <strong>Computer Use Safeguards:</strong> Every automation
                    step requires review; declines cancel the chain immediately.
                    Status updates ensure reviewers see what is happening at all
                    times.
                  </li>
                  <li>
                    <strong>Docs:</strong> See <code>PRIVACY.md</code> for the
                    privacy summary and <code>docs/AppReviewNotes.md</code> for
                    reviewer instructions.
                  </li>
                  </ul>
                  <hr />
                  <h2 id="testing-quality-gates">Testing &amp; Quality Gates</h2>
                  <ul>
                  <li>
                    <strong>Unit &amp; Snapshot Tests:</strong> Run inside Xcode
                    (<code>‚åòU</code>) or via <code>xcodebuild</code> on
                    <code>OpenResponsesTests</code>,
                    <code>StreamingEventDecodingTests</code>, and related
                    targets.
                  </li>
                  <li>
                    <strong>Secret Scan:</strong>
                    <code>python3 scripts/secret_scan.py</code>
                  </li>
                  <li>
                    <strong>Preflight Check:</strong>
                    <code>bash scripts/preflight_check.sh</code> verifies
                    Info.plist usage descriptions and reruns the secret scan.
                  </li>
                  <li>
                    <strong>Manual QA:</strong> Follow
                    <code>docs/PRODUCTION_CHECKLIST.md</code> for streaming,
                    tooling, accessibility, and documentation checks.
                  </li>
                  <li>
                    <strong>API Coverage:</strong> Update
                    <code>docs/api/Full_API_Reference.md</code> when adding
                    request fields, tool types, or event handling.
                  </li>
                  </ul>
                  <hr />
                  <h2 id="release-workflow">Release Workflow</h2>
                <p>
                  The Minimal Viable App-Store Submission (MVAS) plan captures
                  everything needed to submit OpenResponses to TestFlight/App
                  Store with ~6‚Äì12 hours of effort.
                </p>
                <ol>
                  <li>
                    Track progress in
                    <code>docs/MVAS_SUBMISSION_TRACKER.md</code> (checklist +
                    decision log).
                  </li>
                  <li>
                    Ensure privacy copy is current (<code>PRIVACY.md</code>, App
                    Store metadata, <code>docs/AppReviewNotes.md</code>).
                  </li>
                  <li>
                    Run <code>bash scripts/preflight_check.sh</code> to confirm
                    secrets and Info.plist values are clean.
                  </li>
                  <li>Archive in Xcode ‚Üí Organizer ‚Üí Validate/Upload.</li>
                  <li>
                    Invite internal TestFlight testers for the sanity pass
                    (onboarding, chat, computer use).
                  </li>
                  <li>
                    Submit to App Review with the dossier from
                    <code>docs/AppReviewNotes.md</code>.
                  </li>
                  </ol>
                  <hr />
                  <h2 id="documentation-hub">Documentation Hub</h2>
                  <ul>
                  <li>
                    <code>docs/ROADMAP.md</code> ‚Äî phased rollout plan with
                    current status.
                  </li>
                  <li>
                    <code>docs/CASE_STUDY.md</code> ‚Äî architecture narrative
                    including diagrams and streaming lifecycle.
                  </li>
                  <li>
                    <code>docs/api/Full_API_Reference.md</code> ‚Äî field-by-field
                    implementation status for Responses.
                  </li>
                  <li>
                    <code>docs/PRODUCTION_CHECKLIST.md</code> ‚Äî manual QA and
                    release verification steps.
                  </li>
                  <li>
                    <code>docs/Advanced.md</code>, <code>docs/Tools.md</code>,
                    <code>docs/Files.md</code>, <code>docs/Images.md</code> ‚Äî
                    feature-specific how-tos.
                  </li>
                  <li>
                    <code>docs/AppReviewNotes.md</code> ‚Äî one-pager for App
                    Store reviewers.
                  </li>
                  <li><code>Notion/</code> ‚Äî MCP connector setup guides.</li>
                  </ul>
                  <hr />
                  <h2 id="roadmap-snapshot">Roadmap Snapshot</h2>
                  <ul>
                  <li>
                    <strong>Phase¬†1 (Complete):</strong> Multi-modal inputs,
                    full Responses tool coverage, computer-use hardening, vector
                    workflow, observability overhaul.
                  </li>
                  <li>
                    <strong>Phase¬†2 (In Progress):</strong> Conversations API
                    adoption, annotation rendering, cross-device sync, enhanced
                    conversation metadata.
                  </li>
                  <li>
                    <strong>Beyond:</strong> Apple Intelligence integration,
                    richer UI polish, offline caching, and advanced prompt
                    caching (see <code>docs/ROADMAP.md</code>).
                  </li>
                  </ul>
                  <hr />
                  <h2 id="contributing">Contributing</h2>
                  <p>We welcome pull requests aligned with the roadmap.</p>
                  <ol>
                  <li>
                    Fork the repo and branch from <code>main</code> or the
                    active release branch.
                  </li>
                  <li>Implement the change with tests where applicable.</li>
                  <li>
                    Run unit tests and
                    <code>bash scripts/preflight_check.sh</code>.
                  </li>
                  <li>
                    Update relevant docs (<code>docs/</code>,
                    <code>PRIVACY.md</code>, <code>README.md</code>, etc.).
                  </li>
                  <li>
                    Submit a PR describing the change, test evidence, and any
                    roadmap linkage.
                  </li>
                  </ol>
                <p>
                  Please open an issue before large architectural work so we can
                  coordinate on Phase¬†2 priorities.
                </p>
                <hr />
                <h2 id="support">Support</h2>
                <ul>
                  <li>
                    Email:
                    <a href="mailto:support@gunnarguy.com">support@gunnarguy.com</a>
                  </li>
                  <li>
                    Issues:
                    <a href="https://github.com/Gunnarguy/OpenResponses/issues">https://github.com/Gunnarguy/OpenResponses/issues</a>
                  </li>
                  <li>
                    Discussions and roadmap queries: see
                    <code>docs/ROADMAP.md</code> and
                    <code>docs/MVAS_SUBMISSION_TRACKER.md</code>
                  </li>
                  </ul>
                  <hr />
                  <h2 id="license">License</h2>
                <p>
                  MIT ‚Äî see <a href="LICENSE"><code>LICENSE</code></a>.
                </p>
                </div>
                </section>

            <section class="docs-section" id="doc-roadmap">
              <div class="docs-section-header">
                <h2>Roadmap</h2>
                <div class="docs-section-meta">
                  <a href="https://github.com/Gunnarguy/OpenResponses/blob/main/docs/ROADMAP.md" target="_blank" rel="noopener">View
                    source</a>
                  </div>
                  </div>
                  <div class="docs-section-body">
                    <h1 id="openresponses-roadmap">OpenResponses Roadmap</h1>
                    <hr />
                <p>
                  <strong>[2025-11-07] Status Snapshot:</strong> Phase¬†1 is
                  locked: every Responses API tool (computer, code interpreter,
                  file search, image generation, MCP connectors) is
                  production-ready with SwiftUI polish. Recent work introduced
                  the Assistant Thinking panel, revamped prompt ergonomics, and
                  safety upgrades for computer use. Conversations still persist
                  locally while Phase¬†2 work scopes the migration to
                  <code>/v1/conversations</code>.
                </p>
                <p>
                  <strong>To resume:</strong> Review this roadmap,
                  <code>docs/CASE_STUDY.md</code>, and
                  <code>docs/api/Full_API_Reference.md</code> before picking up
                  implementation work. Those documents track architectural
                  context, coverage, and outstanding gaps.
                </p>
                <h2 id="1-objective">1. Objective</h2>
                <p>
                  This document serves as the comprehensive playbook for
                  upgrading the OpenResponses app from its current state to 100%
                  compliance with the latest OpenAI and Apple capabilities. It
                  outlines every feature, API endpoint, and architectural
                  improvement required to create a best-in-class, multimodal AI
                  experience.
                </p>
                <p>
                  This roadmap is organized into five distinct phases, guiding a
                  systematic implementation from core functionality to advanced
                  features and polish.
                </p>
                <hr />
                <h2 id="2-current-status-api-coverage">
                  2. Current Status &amp; API Coverage
                </h2>
                <p>
                  üéâ <strong>MAJOR MILESTONE: Phase 1 Complete!</strong> - All
                  input modalities and advanced tool integrations have been
                  successfully implemented with production-ready quality.
                </p>
                <p>
                  This section provides a comprehensive overview of the current
                  implementation status based on the
                  <code>Full_API_Reference.md</code>.
                </p>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">API Feature Category</th>
                      <th style="text-align: left">Implementation Level</th>
                      <th style="text-align: left">Details</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <strong>Text Input/Output</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Complete</strong>
                      </td>
                      <td style="text-align: left">
                        Full text conversation support with streaming, rich
                        formatting, and comprehensive error handling.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Image Input</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Complete</strong>
                      </td>
                      <td style="text-align: left">
                        Full image selection, base64 encoding, detail level
                        control, and seamless API integration.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>File Input</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Complete</strong>
                      </td>
                      <td style="text-align: left">
                        Full support for both <code>file_id</code> references
                        and direct file uploads with <code>file_data</code>. 43+
                        supported file types.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Audio Input</strong>
                      </td>
                      <td style="text-align: left">
                        ‚ùå <strong>Intentionally Removed</strong>
                      </td>
                      <td style="text-align: left">
                        Audio input UI and APIs were intentionally removed from
                        the app to focus on core functionality.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Basic Tools</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Complete</strong>
                      </td>
                      <td style="text-align: left">
                        Web search, code interpreter (with full artifact
                        support), and file search fully integrated with advanced
                        configurations.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Advanced Tools</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Complete</strong>
                      </td>
                      <td style="text-align: left">
                        Computer Use tool complete with bulletproof error
                        handling; Custom Function tools complete; OpenAI-hosted
                        MCP connectors (Dropbox, Gmail, SharePoint, etc.)
                        supported with OAuth onboarding, plus guided remote MCP
                        templates (including Notion's official hosted server)
                        for advanced deployments.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Streaming Response</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Complete</strong>
                      </td>
                      <td style="text-align: left">
                        Comprehensive handling for text, tool calls, image
                        generation events, and computer use with real-time
                        status updates.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Rich Content Output</strong>
                      </td>
                      <td style="text-align: left">
                        üü° <strong>Partial</strong>
                      </td>
                      <td style="text-align: left">
                        Text rendering is complete with copy functionality;
                        media previews implemented; annotations parsing needs
                        enhancement.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Conversation Management</strong>
                      </td>
                      <td style="text-align: left">
                        ‚ùå <strong>Phase 2 Target</strong>
                      </td>
                      <td style="text-align: left">
                        Local storage complete; backend Conversations API
                        integration planned for Phase 2.
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Advanced Parameters</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Complete</strong>
                      </td>
                      <td style="text-align: left">
                        All API parameters exposed including
                        <code>tool_choice</code>, <code>include</code>,
                        background mode, reasoning controls, and model-specific
                        configurations.
                      </td>
                      </tr>
                      </tbody>
                      </table>
                      <hr />
                <h2 id="3-implementation-playbook-from-partial-to-full-compliance">
                  3. Implementation Playbook: From Partial to Full Compliance
                </h2>
                <p>
                  This playbook details every feature and improvement required
                  to reach 100% API and feature compliance.
                </p>
                <h3 id="phase-1-input-tool-completion-100-complete">
                  üéâ Phase 1: Input &amp; Tool Completion -
                  <strong>100% COMPLETE</strong>
                </h3>
                <p>
                  <strong>Objective:</strong> ‚úÖ <strong>ACHIEVED</strong> -
                  Implemented all remaining input modalities and completed the
                  integration of advanced tools.
                </p>
                <p>
                  <strong>Phase 1 Status: üéâ COMPLETE</strong> - All features
                  implemented with production-ready quality, comprehensive error
                  handling, and full UI integration.
                </p>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">Feature / Improvement</th>
                      <th style="text-align: left">
                        Status &amp; Implementation Evidence
                      </th>
                      <th style="text-align: left">
                        Implementation Details &amp; Architecture
                      </th>
                      <th style="text-align: left">Importance</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <strong>Audio Input</strong>
                      </td>
                      <td style="text-align: left">
                        ‚ùå <strong>Intentionally Removed</strong> - Audio input
                        capabilities were strategically removed from the
                        application scope to focus on core text, image, and file
                        interactions.
                      </td>
                      <td style="text-align: left">
                        ‚Äî Audio input is not part of the application's feature
                        set by design decision.
                      </td>
                      <td style="text-align: left">‚Äî</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Direct File Uploads</strong>
                      </td>
                      <td style="text-align: left">
                        üéâ <strong>100% COMPLETE</strong> - The app fully
                        supports direct file uploads via
                        <code>DocumentPicker.swift</code>, including
                        comprehensive UI for file selection, data reading,
                        base64 encoding, and seamless API integration. Users can
                        upload files directly without pre-uploading to OpenAI's
                        file API.
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>PRODUCTION READY</strong>: Full
                        <code>DocumentPicker.swift</code> implementation with
                        43+ supported file types; ‚úÖ
                        <strong>COMPREHENSIVE UI</strong>:
                        <code>SelectedFilesView.swift</code> for file preview
                        and management; ‚úÖ
                        <strong>SEAMLESS API INTEGRATION</strong>:
                        <code>OpenAIService.buildInputMessages()</code> handles
                        both <code>file_data</code> (direct uploads) and
                        <code>file_id</code> (pre-uploaded files); ‚úÖ
                        <strong>ROBUST FILE HANDLING</strong>: Base64 encoding,
                        filename preservation, error handling, and progress
                        feedback; ‚úÖ <strong>SECURITY</strong>: Sandboxed file
                        access with proper permission handling.
                      </td>
                      <td style="text-align: left">1.0</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Computer Use Tool</strong>
                      </td>
                      <td style="text-align: left">
                        üéâ <strong>100% COMPLETE &amp; BULLETPROOF</strong>. All
                        OpenAI computer actions are now implemented with
                        comprehensive error handling. The feature includes every
                        possible action type (click, double_click, drag,
                        keypress, move, screenshot, scroll, type, wait,
                        navigate) plus graceful handling of unknown actions to
                        prevent any future "invalidActionType" errors. The
                        native iOS <code>ComputerService</code> provides robust
                        browser automation with off-screen WebView, advanced
                        parameter validation, and defensive programming
                        patterns. <strong>Recent enhancements:</strong> Enhanced
                        click implementation with multi-strategy approach for
                        JavaScript-heavy sites, navigate-first enforcement, loop
                        prevention, streaming auto-retry, and a user-in-the-loop
                        safety approval sheet.
                      </td>
                      <td style="text-align: left">
                        - ‚úÖ Computer tool configuration in
                        <code>APICapabilities.swift</code> and
                        <code>buildTools()</code>; - ‚úÖ Settings UI toggle and
                        model compatibility checking; - ‚úÖ API parameter
                        encoding and include options; - ‚úÖ Streaming event
                        handling for computer screenshots and action
                        confirmations; - ‚úÖ Automatic pending call resolution in
                        <code>ChatViewModel</code>; - üéâ
                        <strong>100% ACTION COVERAGE</strong>: All official
                        OpenAI actions implemented (click, double_click, drag,
                        keypress, move, screenshot, scroll, type, wait); - üéâ
                        <strong>BULLETPROOF ERROR HANDLING</strong>: Unknown
                        actions handled gracefully, no more "invalidActionType"
                        errors; - üéâ <strong>PRODUCTION-READY</strong>: Native
                        <code>ComputerService.swift</code> with proper WebView
                        frame initialization (440x956); - üéâ
                        <strong>PRODUCTION-READY</strong>: Single-shot mode
                        prevents infinite loops for screenshot-only requests; -
                        üéâ <strong>PRODUCTION-READY</strong>: Comprehensive
                        error handling and debug logging throughout the
                        pipeline; - üéâ <strong>PRODUCTION-READY</strong>: Status
                        chips display "üñ•Ô∏è Using computer..." during active tool
                        calls; - ‚úÖ <strong>ENHANCED RELIABILITY</strong>:
                        Multi-strategy click implementation (focus, mouse
                        events, direct click, handler triggering) for
                        JavaScript-heavy sites; - ‚úÖ
                        <strong>NAVIGATE-FIRST ENFORCEMENT</strong>: Prevents
                        screenshot-first loops with mandatory navigation rules;
                        - ‚úÖ <strong>STREAMING RESILIENCE</strong>: One-shot
                        auto-retry for transient errors with preserved context;
                        - ‚úÖ <strong>SAFETY APPROVAL UI</strong>: When
                        <code>pending_safety_checks</code> are returned, the app
                        prompts the user to approve or cancel; approvals are
                        sent via <code>acknowledged_safety_checks</code> in the
                        next <code>computer_call_output</code>; - ‚úÖ Full
                        browser automation with ALL computer actions working
                        correctly and defensively.
                      </td>
                      <td style="text-align: left">1.0</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>gpt-image-1 &amp; Streaming Previews</strong>
                      </td>
                      <td style="text-align: left">
                        üéâ <strong>100% COMPLETE</strong> - Full image
                        generation implementation using
                        <code>image_generation</code> tool with production-ready
                        configuration. Streaming events provide real-time
                        feedback during image generation process.
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>PRODUCTION READY</strong>: Complete image
                        generation tool implementation in
                        <code>APICapabilities.swift</code>; ‚úÖ
                        <strong>FULL STREAMING SUPPORT</strong>:
                        <code>ChatViewModel.handleStreamChunk()</code> processes
                        all image generation events; ‚úÖ
                        <strong>REAL-TIME FEEDBACK</strong>: Streaming status
                        displays "üé® Generating Image..." during tool calls; ‚úÖ
                        <strong>COMPREHENSIVE EVENT HANDLING</strong>: Partial
                        preview and completion events fully implemented; ‚úÖ
                        <strong>HIGH-QUALITY OUTPUT</strong>: Configurable
                        quality, size, and format options; ‚úÖ
                        <strong>UI INTEGRATION</strong>: Generated images
                        display immediately in chat with proper formatting.
                      </td>
                      <td style="text-align: left">1.0</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>MCP Tool Enhancements</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Reintroduced</strong> - Official OpenAI MCP
                        connectors (Dropbox, Gmail, Google Calendar, SharePoint,
                        etc.) are available with streamlined OAuth setup. Notion
                        is delivered via the official Model Context Protocol
                        remote server template so users can query, create,
                        update, and delete workspace content without
                        self-hosting.
                      </td>
                      <td style="text-align: left">
                        ‚úÖ OAuth-based onboarding lives in
                        <code>MCPConnectorGalleryView.swift</code>, storing
                        secrets in Keychain and updating
                        <code>Prompt</code> state; ‚úÖ
                        <code>OpenAIService.buildTools()</code> validates
                        connector IDs and injects the authorization header
                        securely; ‚úÖ Notion remote setup auto-fills the official
                        server endpoint per
                        <a
                          href="https://modelcontextprotocol.io/docs/getting-started/intro">https://modelcontextprotocol.io/docs/getting-started/intro</a>
                        and stores the token securely; ‚úÖ Remote MCP templates
                        (e.g., Notion self-hosted) remain in advanced settings
                        for power users who prefer custom infrastructure.
                      </td>
                      <td style="text-align: left">1.0</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Code Interpreter Enhancements</strong>
                      </td>
                      <td style="text-align: left">
                        üéâ <strong>100% COMPLETE</strong> - Full implementation
                        with container types, file preloading, and comprehensive
                        artifact parsing for all 43 file types including logs,
                        text outputs, data files, and documents. Rich UI
                        displays all generated content with copy functionality
                        and expandable views.
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>PRODUCTION READY</strong>: Complete code
                        interpreter tool with all configuration options; ‚úÖ
                        <strong>CONTAINER SELECTION</strong>: Full UI in
                        <code>SettingsView</code> for auto/secure/gpu container
                        options; ‚úÖ <strong>FILE PRELOADING</strong>:
                        Comprehensive <code>file_ids</code> parameter support
                        with comma-separated input; ‚úÖ
                        <strong>ARTIFACT PARSING</strong>: Full parsing for all
                        43 supported file types (logs, CSV, JSON, Python files,
                        documents); ‚úÖ <strong>RICH UI</strong>:
                        <code>ArtifactView.swift</code> with expandable content,
                        copy functionality, and proper MIME type handling; ‚úÖ
                        <strong>STREAMING STATUS</strong>: Real-time feedback
                        showing code execution and artifact processing phases;
                        ‚úÖ <strong>ERROR HANDLING</strong>: Comprehensive error
                        states and user feedback for failed executions.
                      </td>
                      <td style="text-align: left">1.0</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>File Search Enhancements</strong>
                      </td>
                      <td style="text-align: left">
                        üéâ <strong>100% COMPLETE</strong> - Multi-vector-store
                        search is fully implemented with comprehensive UI for
                        managing multiple vector stores. Users can search across
                        multiple knowledge bases simultaneously with advanced
                        filtering and organization.
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>PRODUCTION READY</strong>: Complete
                        multi-vector-store implementation with comma-separated
                        vector store ID input; ‚úÖ
                        <strong>TYPE-SAFE API</strong>:
                        <code>APICapabilities.Tool.fileSearch</code> properly
                        handles arrays of vector store IDs; ‚úÖ
                        <strong>COMPREHENSIVE UI</strong>: File search
                        configuration UI accepts and validates comma-separated
                        vector store IDs; ‚úÖ <strong>ROBUST SEARCH</strong>:
                        Simultaneous search across multiple vector stores with
                        result aggregation; ‚úÖ <strong>USER EXPERIENCE</strong>:
                        Clear feedback for search operations and result
                        organization.
                      </td>
                      <td style="text-align: left">1.0</td>
                      </tr>
                      </tbody>
                      </table>
                      <hr />
                <h3 id="model-compatibility-ultra-strict-mode-2025-09-13">
                  Model Compatibility &amp; Ultra-strict Mode (2025-09-13)
                </h3>
                <ul>
                  <li>
                    <strong>Computer-use is only available on the
                      <code>computer-use-preview</code> model.</strong>
                    For gpt-4o, gpt-4-turbo, and others, the tool is disabled by
                    design (see <code>ModelCompatibilityService.swift</code>).
                  </li>
                  <li>
                    <strong>Ultra-strict mode</strong> disables all app-side
                    helpers (pre-navigation, intent-aware search, click-by-text,
                    loop-prevention) for purist/diagnostic use. Toggle in
                    Settings ‚Üí Debugging.
                  </li>
                  <li>
                    All recent changes are documented in
                    <code>CASE_STUDY.md</code> and
                    <code>Full_API_Reference.md</code> for easy resumption.
                  </li>
                  </ul>
                <h3 id="phase-2-conversation-backend-sync">
                  Phase 2: Conversation &amp; Backend Sync
                </h3>
                <p>
                  <strong>Objective:</strong> Replace the local conversation
                  storage system with the official OpenAI Conversations API for
                  cross-device sync and persistence.
                </p>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">Feature / Improvement</th>
                      <th style="text-align: left">Rationale &amp; Evidence</th>
                      <th style="text-align: left">
                        Required Actions &amp; Affected Files/Classes
                      </th>
                      <th style="text-align: left">Importance</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <strong>Backend-Managed Conversations</strong>
                      </td>
                      <td style="text-align: left">
                        The app uses local storage only. Without server sync,
                        conversations are not accessible across devices.
                      </td>
                      <td style="text-align: left">
                        - Implement <code>createConversation</code>,
                        <code>listConversations</code>,
                        <code>getConversation</code>,
                        <code>updateConversation</code>, and
                        <code>deleteConversation</code> in
                        <code>OpenAIService</code>.<br />- Update
                        <code>ConversationStorageService</code> to fetch and
                        sync conversations with the backend.<br />- Add offline
                        fallback to use a local cache.
                      </td>
                      <td style="text-align: left">0.9</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Conversation-Level Metadata</strong>
                      </td>
                      <td style="text-align: left">
                        The Conversations API supports metadata for custom tags,
                        topics, or user preferences.
                      </td>
                      <td style="text-align: left">
                        - Update the <code>Conversation</code> data model to
                        include a <code>metadata</code> dictionary.<br />-
                        Provide UI for tagging and searching conversations in
                        <code>ConversationListView</code>.
                      </td>
                      <td style="text-align: left">0.4</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Conversation State &amp;
                          <code>store</code> Parameter</strong>
                      </td>
                      <td style="text-align: left">
                        Use the <code>conversation</code> object instead of
                        <code>previous_response_id</code> for state. Allow
                        disabling storage via <code>store: false</code>.
                      </td>
                      <td style="text-align: left">
                        - Adjust
                        <code>OpenAIService.buildRequestObject()</code> to
                        include the full conversation object or ID.<br />-
                        Provide a toggle for the <code>store</code> parameter in
                        <code>SettingsView</code> for privacy-sensitive
                        sessions.
                      </td>
                      <td style="text-align: left">0.6</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Hierarchical Roles</strong>
                      </td>
                      <td style="text-align: left">
                        The API introduces new roles: <code>platform</code>,
                        <code>system</code>, <code>developer</code>. Developer
                        messages override user content.
                      </td>
                      <td style="text-align: left">
                        - Modify <code>InputMessage</code> to support roles
                        beyond <code>system</code>/<code>user</code>.<br />-
                        Update UI to allow creation of
                        <code>developer</code> and <code>system</code> messages,
                        perhaps in an advanced settings screen.
                      </td>
                      <td style="text-align: left">0.5</td>
                      </tr>
                      </tbody>
                      </table>
                <h3 id="phase-3-uiux-apple-framework-integration">
                  Phase 3: UI/UX &amp; Apple Framework Integration
                </h3>
                <p>
                  <strong>Objective:</strong> Modernize the user interface and
                  integrate powerful on-device features from the latest Apple
                  frameworks.
                </p>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">Feature / Improvement</th>
                      <th style="text-align: left">Rationale &amp; Evidence</th>
                      <th style="text-align: left">
                        Required Actions &amp; Affected Files/Classes
                      </th>
                      <th style="text-align: left">Importance</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <strong>Real-time Streaming Feedback</strong>
                      </td>
                      <td style="text-align: left">
                        ‚úÖ <strong>Complete</strong>. Users need visual feedback
                        during streaming to prevent "frozen" perception.
                      </td>
                      <td style="text-align: left">
                        - ‚úÖ Implemented blinking typing cursor in
                        <code>MessageBubbleView</code> during assistant
                        streaming.<br />- ‚úÖ Added live token estimation with
                        per-message and conversation-level counters.<br />- ‚úÖ
                        Created detailed activity feed
                        (<code>ActivityFeedView</code>) with real-time updates
                        during streaming events.<br />- ‚úÖ Added expandable
                        Details panel showing connection status, tool usage, and
                        reasoning phases.<br />- ‚úÖ Refactored
                        <code>ChatView</code> for better SwiftUI performance and
                        compiler optimization.
                      </td>
                      <td style="text-align: left">1.0</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Liquid Glass Design</strong>
                      </td>
                      <td style="text-align: left">
                        Adopt Apple's new <code>Liquid Glass</code> design
                        language for a modern, translucent UI.
                      </td>
                      <td style="text-align: left">
                        - Recompile with the latest Xcode.<br />- Apply
                        <code>glassEffect()</code> modifiers to toolbars,
                        navigation bars, and chat bubbles.<br />- Provide
                        per-view opt-outs for readability.
                      </td>
                      <td style="text-align: left">0.6</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Live Translation &amp; Visual Intelligence</strong>
                      </td>
                      <td style="text-align: left">
                        Apple's on-device frameworks for Live Translation and
                        Visual Intelligence are not integrated.
                      </td>
                      <td style="text-align: left">
                        - Use <code>FoundationModels</code> or
                        <code>Vision</code> frameworks for on-device
                        translation.<br />- Allow users to share screenshots,
                        detect objects, and feed results into a custom tool.
                      </td>
                      <td style="text-align: left">0.6</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Rich Text Editing</strong>
                      </td>
                      <td style="text-align: left">
                        SwiftUI now includes a built-in <code>WebView</code> and
                        rich text editing via <code>AttributedString</code>.
                      </td>
                      <td style="text-align: left">
                        - Replace custom web view implementations with the
                        native <code>WebView</code>.<br />- Use rich text
                        editing for the user input view to support bold,
                        italics, and lists.
                      </td>
                      <td style="text-align: left">0.4</td>
                      </tr>
                      </tbody>
                      </table>
                <h3 id="phase-4-on-device-real-time-capabilities">
                  Phase 4: On-Device &amp; Real-Time Capabilities
                </h3>
                <p>
                  <strong>Objective:</strong> Integrate on-device AI for
                  low-latency, offline-capable features and explore real-time
                  voice interactions.
                </p>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">Feature / Improvement</th>
                      <th style="text-align: left">Rationale &amp; Evidence</th>
                      <th style="text-align: left">
                        Required Actions &amp; Affected Files/Classes
                      </th>
                      <th style="text-align: left">Importance</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <strong>On-Device AI Integration</strong>
                      </td>
                      <td style="text-align: left">
                        Apple‚Äôs on-device models offer 50-200ms latency and
                        offline operation, significantly improving user
                        retention.
                      </td>
                      <td style="text-align: left">
                        - Integrate Apple‚Äôs
                        <code>FoundationModels</code> framework for local
                        summarization and text generation.<br />- Implement a
                        fallback mechanism: on-device first, then cloud via
                        OpenAI.<br />- Add UI in <code>SettingsView</code> to
                        select processing mode (on-device vs. cloud).
                      </td>
                      <td style="text-align: left">0.9</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Offline Conversation Caching</strong>
                      </td>
                      <td style="text-align: left">
                        Store conversation messages locally for offline viewing
                        and queue requests to send when back online.
                      </td>
                      <td style="text-align: left">
                        - Extend <code>ConversationStorageService</code> to
                        store unsent messages and sync when the network is
                        available.<br />- Provide offline indicators and sync
                        status in the UI.
                      </td>
                      <td style="text-align: left">0.5</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Real-time API / gpt-realtime</strong>
                      </td>
                      <td style="text-align: left">
                        OpenAI‚Äôs new realtime API provides speech-to-speech
                        interactions. The app has no voice features.
                      </td>
                      <td style="text-align: left">
                        - Voice features are out of scope for this app.<br />-
                        Future consideration may include realtime text
                        interactions only.<br />- Integrate
                        <code>gpt-realtime</code> models when available (text
                        focus).
                      </td>
                      <td style="text-align: left">0.7</td>
                      </tr>
                      </tbody>
                      </table>
                <h3 id="phase-5-privacy-security-analytics">
                  Phase 5: Privacy, Security &amp; Analytics
                </h3>
                <p>
                  <strong>Objective:</strong> Harden the application with robust
                  privacy controls, improved error handling, and comprehensive
                  analytics.
                </p>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">Feature / Improvement</th>
                      <th style="text-align: left">Rationale &amp; Evidence</th>
                      <th style="text-align: left">
                        Required Actions &amp; Affected Files/Classes
                      </th>
                      <th style="text-align: left">Importance</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <strong>Encryption &amp; Zero-Data Retention</strong>
                      </td>
                      <td style="text-align: left">
                        73% of users worry about privacy. The API supports
                        <code>reasoning.encrypted_content</code> and
                        <code>store: false</code>.
                      </td>
                      <td style="text-align: left">
                        - Add a toggle to enable encrypted reasoning; handle
                        keys via <code>KeychainService</code>.<br />- Provide a
                        <code>store</code> switch in
                        <code>SettingsView</code> to prevent storing
                        conversations on OpenAI‚Äôs servers.
                      </td>
                      <td style="text-align: left">0.6</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Graceful Error Handling</strong>
                      </td>
                      <td style="text-align: left">
                        Provide user-friendly messages for common HTTP errors
                        (400, 401, 429, 500).
                      </td>
                      <td style="text-align: left">
                        - Map server error codes to localized, user-friendly
                        messages.<br />- Display actionable suggestions (e.g.,
                        re-authenticate on 401).
                      </td>
                      <td style="text-align: left">0.5</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Concurrency &amp; Memory Optimization</strong>
                      </td>
                      <td style="text-align: left">
                        Use modern Swift Concurrency features for efficient
                        memory management and performance.
                      </td>
                      <td style="text-align: left">
                        - Refactor streaming handlers to use
                        <code>AsyncSequence</code> and other concurrency
                        primitives.<br />- Optimize memory usage for large
                        conversations.
                      </td>
                      <td style="text-align: left">0.5</td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Comprehensive Analytics</strong>
                      </td>
                      <td style="text-align: left">
                        Extend basic analytics to record tool usage frequency,
                        latency, token usage, and error rates.
                      </td>
                      <td style="text-align: left">
                        - Add instrumentation points in
                        <code>ChatViewModel</code> for each API call and
                        event.<br />- Use privacy-preserving analytics
                        techniques.
                      </td>
                      <td style="text-align: left">0.4</td>
                      </tr>
                      </tbody>
                      </table>
                      <hr />
                <h2 id="4-detailed-api-endpoint-implementation-plan">
                  4. Detailed API Endpoint Implementation Plan
                </h2>
                <h3 id="responses-api-v1responses">
                  Responses API (<code>/v1/responses</code>)
                </h3>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">Endpoint</th>
                      <th style="text-align: left">Method</th>
                      <th style="text-align: left">Description &amp; Tasks</th>
                      <th style="text-align: left">Current Status</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <code>/v1/responses</code>
                      </td>
                      <td style="text-align: left">POST</td>
                      <td style="text-align: left">
                        Create a new model response. Already implemented but
                        must be expanded to support all parameters from the
                        playbook.
                      </td>
                      <td style="text-align: left"><strong>Partial</strong></td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <code>/v1/responses/{id}</code>
                      </td>
                      <td style="text-align: left">GET</td>
                      <td style="text-align: left">
                        Retrieve a response. Needed for background-mode polling
                        and error recovery. Add <code>getResponse(id:)</code> in
                        <code>OpenAIService</code>.
                      </td>
                      <td style="text-align: left">
                        <strong>Not Implemented</strong>
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <code>/v1/responses/{id}</code>
                      </td>
                      <td style="text-align: left">DELETE</td>
                      <td style="text-align: left">
                        Delete a response. Rarely needed but required for full
                        API support.
                      </td>
                      <td style="text-align: left">
                        <strong>Not Implemented</strong>
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <code>/v1/responses/{id}/cancel</code>
                      </td>
                      <td style="text-align: left">POST</td>
                      <td style="text-align: left">
                        Cancel a background response. Add UI control and call
                        <code>cancelResponse(id:)</code>.
                      </td>
                      <td style="text-align: left">
                        <strong>Not Implemented</strong>
                      </td>
                      </tr>
                      </tbody>
                      </table>
                <h3 id="conversations-api-v1conversations">
                  Conversations API (<code>/v1/conversations</code>)
                </h3>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">Endpoint</th>
                      <th style="text-align: left">Method</th>
                      <th style="text-align: left">Description &amp; Tasks</th>
                      <th style="text-align: left">Current Status</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <code>/v1/conversations</code>
                      </td>
                      <td style="text-align: left">POST</td>
                      <td style="text-align: left">
                        Create a new conversation. Replace local creation in
                        <code>ConversationStorageService</code> with a network
                        call.
                      </td>
                      <td style="text-align: left">
                        <strong>Not Implemented</strong>
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <code>/v1/conversations</code>
                      </td>
                      <td style="text-align: left">GET</td>
                      <td style="text-align: left">
                        List conversations. Replace local storage retrieval with
                        a network call.
                      </td>
                      <td style="text-align: left">
                        <strong>Not Implemented</strong>
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <code>/v1/conversations/{id}</code>
                      </td>
                      <td style="text-align: left">GET</td>
                      <td style="text-align: left">
                        Retrieve conversation history. Use when a user selects a
                        conversation.
                      </td>
                      <td style="text-align: left">
                        <strong>Not Implemented</strong>
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <code>/v1/conversations/{id}</code>
                      </td>
                      <td style="text-align: left">POST</td>
                      <td style="text-align: left">
                        Update a conversation (e.g., rename). Add editing UI and
                        a network call.
                      </td>
                      <td style="text-align: left">
                        <strong>Not Implemented</strong>
                      </td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <code>/v1/conversations/{id}</code>
                      </td>
                      <td style="text-align: left">DELETE</td>
                      <td style="text-align: left">
                        Delete a conversation from the backend. Add a UI action
                        and network call.
                      </td>
                      <td style="text-align: left">
                        <strong>Not Implemented</strong>
                      </td>
                      </tr>
                      </tbody>
                      </table>
                <h3 id="streaming-events-server-sent-events">
                  Streaming Events (Server-Sent Events)
                </h3>
                <table>
                  <thead>
                    <tr>
                      <th style="text-align: left">Event Category</th>
                      <th style="text-align: left">Description &amp; Tasks</th>
                      <th style="text-align: left">Current Status</th>
                      </tr>
                      </thead>
                      <tbody>
                        <tr>
                      <td style="text-align: left">
                        <strong>Core Events</strong>
                      </td>
                      <td style="text-align: left">
                        The app must handle all SSE events:
                        <code>response.created</code>,
                        <code>response.in_progress</code>,
                        <code>response.completed</code>,
                        <code>response.failed</code>, etc.
                      </td>
                      <td style="text-align: left"><strong>Partial</strong></td>
                      </tr>
                      <tr>
                      <td style="text-align: left">
                        <strong>Nested Events</strong>
                      </td>
                      <td style="text-align: left">
                        Expand the <code>StreamingEvent</code> model to decode
                        all event types, like <code>tool_call.started</code> and
                        <code>reasoning.started</code>.
                      </td>
                      <td style="text-align: left"><strong>Partial</strong></td>
                      </tr>
                      </tbody>
                      </table>
                      <hr />
                <h2 id="5-supporting-documentation">
                  5. Supporting Documentation
                </h2>
                <p>
                  For more specific details, refer to the following documents in
                  the repository:
                </p>
                <ul>
                  <li>
                    <strong><code>/docs/api/Full_API_Reference.md</code></strong>: A detailed, field-level analysis of the app's current
                    API
                    implementation.
                  </li>
                  <li>
                    <strong><code>/docs/Tools.md</code></strong>: A guide to using tools like Web Search, File Search, and
                    Function Calling.
                  </li>
                  <li>
                    <strong><code>/docs/Images.md</code></strong>: A guide to image generation and vision capabilities.
                  </li>
                  <li>
                    <strong><code>/docs/Advanced.md</code></strong>: A guide to advanced features like streaming and
                    structured outputs.
                  </li>
                  <li>
                    <strong><code>/docs/PromptingGuide.md</code></strong>: Best practices for writing effective prompts.
                  </li>
                  <li>
                    <strong><code>/docs/PRODUCTION_CHECKLIST.md</code></strong>: A comprehensive guide for pre-release validation.
                  </li>
                  <li>
                    <strong><code>/docs/PRIVACY_POLICY.md</code></strong>: The official privacy policy for the application.
                  </li>
                  <li>
                    <strong><code>.github/copilot-instructions.md</code></strong>: AI coding conventions for contributing to the codebase.
                  </li>
                  </ul>
                  </div>
                  </section>

            <section class="docs-section docs-section-missing" id="doc-architecture">
              <div class="docs-section-header">
                <h2>Architecture</h2>
                <div class="docs-section-meta">
                  <a href="https://github.com/Gunnarguy/OpenResponses/blob/main/docs/ARCHITECTURE.md" target="_blank" rel="noopener">View
                    source</a>
                  <span class="docs-status-pill">Missing</span>
                  </div>
                  </div>
                  <div class="docs-section-body">
                <p><strong>Could not load this document.</strong></p>
                <p>Path: <code>docs/ARCHITECTURE.md</code></p>
                <p>HTTP Error 404: Not Found</p>
                </div>
                </section>
          </article>
        </div>
      </div>
    </main>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        const navToggle = document.querySelector(".nav-toggle");
        const navMenu = document.querySelector(".nav-menu");
      if (navToggle && navMenu) {
        navToggle.addEventListener("click", () => {
          navMenu.classList.toggle("nav-menu-active");
          navToggle.classList.toggle("nav-toggle-active");
        });
      }

      const normalizeMermaid = () => {
        document
          .querySelectorAll("pre > code.language-mermaid")
          .forEach((block) => {
            const parent = block.parentElement;
            const wrapper = document.createElement("div");
            wrapper.className = "mermaid";
            wrapper.textContent = block.textContent || "";
            parent.replaceWith(wrapper);
          });
        if (window.mermaid) {
          mermaid.initialize({ startOnLoad: true, theme: "neutral" });
          mermaid.init(undefined, document.querySelectorAll(".mermaid"));
        }
      };

        document.addEventListener("DOMContentLoaded", normalizeMermaid);
    </script>
  </body>
</html>
